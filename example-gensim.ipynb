{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример использования библиотеки gensim для тематического моделирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from kullback_leibler import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import io \n",
    "#f = io.open(\"vocab.bow.txt\", mode=\"r\", encoding=\"utf8\") \n",
    "#f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Импортируем данные в формте UCI Bag of Words\n",
    "data = corpora.UciCorpus(\"docword.bow.txt\", \"vocab.bow.txt\")\n",
    "dictionary = data.create_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for k in dictionary.keys():\n",
    "    #v = v.decode('utf8')\n",
    "#    dictionary[k] = dictionary[k].decode('utf8')\n",
    "#     print(dictionary.k, k)\n",
    "\n",
    "# new1 = {(k, v.decode('utf8')) for k, v in dictionary.items()}\n",
    "#print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.9 s\n"
     ]
    }
   ],
   "source": [
    "# обучение модель\n",
    "%time ldamodel = models.ldamodel.LdaModel(data, id2word=dictionary, num_topics=5, passes=20, alpha=1.25, eta=1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "ldamodel.save(\"ldamodel_xkcd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Загрузка модели\n",
    "ldamodel = models.ldamodel.LdaModel.load(\"ldamodel_xkcd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : 0.002*\"b'\\xd0\\xb1\\xd1\\x83\\xd1\\x84\\xd0\\xb5\\xd1\\x80'\" + 0.002*\"b'\\xd1\\x84\\xd0\\xb0\\xd0\\xb9\\xd0\\xbb\\xd0\\xbe\\xd0\\xb2\\xd1\\x8b\\xd0\\xb9'\" + 0.002*\"b'\\xd1\\x80\\xd0\\xb0\\xd0\\xb1\\xd0\\xbe\\xd1\\x82\\xd0\\xb0\\xd1\\x82\\xd1\\x8c'\" + 0.002*\"b'\\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd1\\x86\\xd0\\xb5\\xd1\\x81\\xd1\\x81'\" + 0.002*\"b'\\xd0\\xb4\\xd0\\xb0\\xd1\\x82\\xd1\\x8c'\" + 0.002*\"b'\\xd1\\x84\\xd0\\xb0\\xd0\\xb9\\xd0\\xbb'\" + 0.002*\"b'\\xd0\\xb1\\xd0\\xbe\\xd0\\xbb\\xd1\\x8c\\xd1\\x88\\xd0\\xbe\\xd0\\xb9'\" + 0.002*\"b'\\xd0\\xbc\\xd0\\xbe\\xd1\\x87\\xd1\\x8c'\" + 0.002*\"b'\\xd1\\x81\\xd0\\xb8\\xd1\\x81\\xd1\\x82\\xd0\\xb5\\xd0\\xbc\\xd0\\xb0'\" + 0.002*\"b'\\xd1\\x81\\xd0\\xb4\\xd0\\xb5\\xd0\\xbb\\xd0\\xb0\\xd1\\x82\\xd1\\x8c'\" \n",
      "\n",
      "Topic 1 : 0.003*\"b'\\xd1\\x82\\xd0\\xbe\\xd0\\xba\\xd0\\xb5\\xd0\\xbd'\" + 0.002*\"b'\\xd0\\xb4\\xd0\\xbe\\xd0\\xbc'\" + 0.002*\"b'\\xd0\\xbc\\xd0\\xbe\\xd1\\x87\\xd1\\x8c'\" + 0.002*\"b'\\xd0\\xba\\xd0\\xbe\\xd0\\xbd\\xd1\\x82\\xd1\\x80\\xd0\\xb0\\xd0\\xba\\xd1\\x82'\" + 0.002*\"b'\\xd0\\xbc\\xd0\\xb5\\xd1\\x81\\xd1\\x8f\\xd1\\x86'\" + 0.002*\"b'\\xd1\\x80\\xd0\\xb5\\xd0\\xb1\\xd1\\x91\\xd0\\xbd\\xd0\\xbe\\xd0\\xba'\" + 0.002*\"b'\\xd1\\x88\\xd1\\x82\\xd0\\xb0\\xd1\\x82'\" + 0.002*\"b'\\xd0\\xbd\\xd0\\xb0\\xd0\\xbf\\xd1\\x80\\xd0\\xb8\\xd0\\xbc\\xd0\\xb5\\xd1\\x80'\" + 0.002*\"b'\\xd0\\xb0\\xd0\\xbb\\xd0\\xb0\\xd0\\xb1\\xd0\\xb0\\xd0\\xbc\\xd0\\xb0'\" + 0.002*\"b'\\xd0\\xbc\\xd0\\xb0\\xd0\\xb3\\xd0\\xb0\\xd0\\xb7\\xd0\\xb8\\xd0\\xbd'\" \n",
      "\n",
      "Topic 2 : 0.001*\"b'\\xd1\\x80\\xd0\\xb5\\xd0\\xb7\\xd1\\x83\\xd0\\xbb\\xd1\\x8c\\xd1\\x82\\xd0\\xb0\\xd1\\x82'\" + 0.001*\"b'\\xd0\\xbf\\xd0\\xb0\\xd0\\xbb\\xd0\\xba\\xd0\\xb0'\" + 0.001*\"b'\\xd0\\xbf\\xd1\\x80\\xd0\\xbe\\xd0\\xb3\\xd1\\x80\\xd0\\xb0\\xd0\\xbc\\xd0\\xbc\\xd0\\xb8\\xd1\\x80\\xd0\\xbe\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5'\" + 0.001*\"b'\\xd0\\xb3\\xd0\\xb0\\xd0\\xbb\\xd0\\xba\\xd0\\xb0'\" + 0.001*\"b'\\xd0\\xbf\\xd0\\xb5\\xd1\\x80\\xd0\\xb8\\xd0\\xbc\\xd0\\xb5\\xd1\\x82\\xd1\\x80'\" + 0.001*\"b'\\xd0\\xbf\\xd0\\xbe\\xd0\\xb7\\xd0\\xb4\\xd1\\x80\\xd0\\xb0\\xd0\\xb2\\xd0\\xbb\\xd1\\x8f\\xd1\\x82\\xd1\\x8c'\" + 0.001*\"b'\\xd1\\x80\\xd0\\xb0\\xd0\\xb2\\xd0\\xbd\\xd1\\x8b\\xd0\\xb9'\" + 0.001*\"b'\\xd0\\xbe\\xd1\\x81\\xd0\\xbd\\xd0\\xbe\\xd0\\xb2\\xd0\\xb0'\" + 0.001*\"b'\\xd0\\xba\\xd0\\xb2\\xd0\\xb0\\xd0\\xbd\\xd1\\x82\\xd0\\xbe\\xd0\\xb2\\xd1\\x8b\\xd0\\xb9'\" + 0.001*\"b'\\xd0\\xbf\\xd0\\xbe\\xd0\\xb4\\xd0\\xb5\\xd0\\xbb\\xd0\\xb8\\xd1\\x82\\xd1\\x8c\\xd1\\x81\\xd1\\x8f'\" \n",
      "\n",
      "Topic 3 : 0.006*\"b'\\xd0\\xbe\\xd0\\xb1\\xd1\\x83\\xd1\\x87\\xd0\\xb5\\xd0\\xbd\\xd0\\xb8\\xd0\\xb5'\" + 0.006*\"b'\\xd1\\x81\\xd0\\xb5\\xd1\\x82\\xd1\\x8c'\" + 0.006*\"b'\\xd0\\xb7\\xd0\\xb0\\xd0\\xb4\\xd0\\xb0\\xd1\\x87\\xd0\\xb0'\" + 0.004*\"b'\\xd0\\xb4\\xd0\\xb0\\xd1\\x82\\xd1\\x8c'\" + 0.004*\"b'\\xd0\\xbc\\xd0\\xbe\\xd1\\x87\\xd1\\x8c'\" + 0.004*\"b'\\xd0\\xbd\\xd0\\xb5\\xd0\\xb9\\xd1\\x80\\xd0\\xbe\\xd0\\xbd\\xd0\\xbd\\xd1\\x8b\\xd0\\xb9'\" + 0.004*\"b'\\xd1\\x87\\xd0\\xb5\\xd0\\xbb\\xd0\\xbe\\xd0\\xb2\\xd0\\xb5\\xd0\\xba'\" + 0.003*\"b'\\xd0\\xb8\\xd1\\x81\\xd0\\xbf\\xd0\\xbe\\xd0\\xbb\\xd1\\x8c\\xd0\\xb7\\xd0\\xbe\\xd0\\xb2\\xd0\\xb0\\xd1\\x82\\xd1\\x8c'\" + 0.003*\"b'\\xd0\\xba\\xd0\\xbd\\xd0\\xb8\\xd0\\xb3\\xd0\\xb0'\" + 0.003*\"b'\\xd0\\xb3\\xd0\\xbb\\xd1\\x83\\xd0\\xb1\\xd0\\xb8\\xd0\\xbd\\xd0\\xbd\\xd1\\x8b\\xd0\\xb9'\" \n",
      "\n",
      "Topic 4 : 0.009*\"b'\\xd1\\x80\\xd0\\xb0\\xd0\\xbc\\xd0\\xb1\\xd0\\xbb\\xd0\\xb5\\xd1\\x80'\" + 0.008*\"b'\\xd0\\xb8\\xd0\\xbd\\xd1\\x82\\xd0\\xb5\\xd1\\x80\\xd0\\xbd\\xd0\\xb5\\xd1\\x82'\" + 0.007*\"b'\\xd0\\xba\\xd0\\xbe\\xd0\\xbc\\xd0\\xbf\\xd0\\xb0\\xd0\\xbd\\xd0\\xb8\\xd1\\x8f'\" + 0.004*\"b'\\xd0\\xb4\\xd0\\xb5\\xd0\\xbd\\xd1\\x8c\\xd0\\xb3\\xd0\\xb0'\" + 0.004*\"b'\\xd0\\xbc\\xd0\\xb8\\xd0\\xbb\\xd0\\xbb\\xd0\\xb8\\xd0\\xbe\\xd0\\xbd'\" + 0.003*\"b'\\xd0\\xb2\\xd0\\xb8\\xd0\\xba\\xd1\\x82\\xd0\\xbe\\xd1\\x80'\" + 0.003*\"b'\\xd0\\xb4\\xd0\\xbe\\xd0\\xbb\\xd0\\xbb\\xd0\\xb0\\xd1\\x80'\" + 0.003*\"b'\\xd1\\x81\\xd1\\x82\\xd0\\xb0\\xd1\\x82\\xd1\\x8c'\" + 0.003*\"b'\\xd0\\xb2\\xd1\\x80\\xd0\\xb5\\xd0\\xbc\\xd1\\x8f'\" + 0.003*\"b'\\xd1\\x81\\xd0\\xbf\\xd1\\x83\\xd1\\x82\\xd0\\xbd\\xd0\\xb8\\xd0\\xba'\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import io\n",
    "\n",
    "#output = io.open(\"vocab.bow.txt\", mode=\"r\", encoding=\"utf8\")\n",
    "# выводим топы слов\n",
    "with io.open('output.txt', 'w') as file:\n",
    "    for t, top_words in ldamodel.print_topics(num_topics=10, num_words=10):\n",
    "        print (\"Topic\", t, \":\", top_words, \"\\n\")#bytes(top_words, 'cp1251').decode('cp1251'))#top_words, \"\\n\")\n",
    "        file.write(top_words)\n",
    "#f = io.open(\"output.txt\", mode=\"r\", encoding=\"utf8\") \n",
    "#f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "all_words = [] #все слова из тем \n",
    "for i in range(5):\n",
    "    rus_words = [] #для сохранения русского варианта\n",
    "    words = ldamodel.show_topic(i, 10)\n",
    "    words = np.take(words, [0], axis =1)\n",
    "    for w in words:\n",
    "        rus_words.append(w[0].decode('utf8'))\n",
    "    all_words.append(rus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тема 0 : ['буфер', 'файловый', 'работать', 'процесс', 'дать', 'файл', 'большой', 'мочь', 'система', 'сделать']\n",
      "тема 1 : ['токен', 'дом', 'мочь', 'контракт', 'месяц', 'ребёнок', 'штат', 'например', 'алабама', 'магазин']\n",
      "тема 2 : ['результат', 'палка', 'программирование', 'галка', 'периметр', 'поздравлять', 'равный', 'основа', 'квантовый', 'поделиться']\n",
      "тема 3 : ['обучение', 'сеть', 'задача', 'дать', 'мочь', 'нейронный', 'человек', 'использовать', 'книга', 'глубинный']\n",
      "тема 4 : ['рамблер', 'интернет', 'компания', 'деньга', 'миллион', 'виктор', 'доллар', 'стать', 'время', 'спутник']\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(all_words):\n",
    "    print(\"тема\", i, \":\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.406884957\n"
     ]
    }
   ],
   "source": [
    "# Вычисляем логарифм перплексии и немного преобразуем, чтобы привести к общепринятому виду\n",
    "perplexity = ldamodel.log_perplexity(list(data))\n",
    "print (2**(-perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1041644453025139"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perp = ldamodel.bound(data)\n",
    "2**(-perp/float(87409))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>привет</td>\n",
       "      <td>валентин</td>\n",
       "      <td>распознавание</td>\n",
       "      <td>привет</td>\n",
       "      <td>делиться</td>\n",
       "      <td>звать</td>\n",
       "      <td>слышимый</td>\n",
       "      <td>неважно</td>\n",
       "      <td>пять</td>\n",
       "      <td>сердечно</td>\n",
       "      <td>привет</td>\n",
       "      <td>пара</td>\n",
       "      <td>продолжение</td>\n",
       "      <td>довольно</td>\n",
       "      <td>недавний</td>\n",
       "      <td>вообще</td>\n",
       "      <td>очередной</td>\n",
       "      <td>привет</td>\n",
       "      <td>попытка</td>\n",
       "      <td>компания</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>хабра</td>\n",
       "      <td>евтюхина</td>\n",
       "      <td>изображение</td>\n",
       "      <td>хабра</td>\n",
       "      <td>рассказ</td>\n",
       "      <td>иван</td>\n",
       "      <td>сверточный</td>\n",
       "      <td>зарекомендовать</td>\n",
       "      <td>назад</td>\n",
       "      <td>приветствовать</td>\n",
       "      <td>пост</td>\n",
       "      <td>месяц</td>\n",
       "      <td>серия</td>\n",
       "      <td>специалист</td>\n",
       "      <td>статья</td>\n",
       "      <td>нормальный</td>\n",
       "      <td>история</td>\n",
       "      <td>одесситка</td>\n",
       "      <td>найти</td>\n",
       "      <td>наиболее</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>сегодня</td>\n",
       "      <td>автор</td>\n",
       "      <td>классический</td>\n",
       "      <td>звать</td>\n",
       "      <td>опыт</td>\n",
       "      <td>серов</td>\n",
       "      <td>нейронный</td>\n",
       "      <td>компания</td>\n",
       "      <td>нейронный</td>\n",
       "      <td>хабравчанин</td>\n",
       "      <td>хотеть</td>\n",
       "      <td>назад</td>\n",
       "      <td>рассказ</td>\n",
       "      <td>индустрия</td>\n",
       "      <td>переезд</td>\n",
       "      <td>общемировой</td>\n",
       "      <td>переезд</td>\n",
       "      <td>назад</td>\n",
       "      <td>работа</td>\n",
       "      <td>известный</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>рассказать</td>\n",
       "      <td>канал</td>\n",
       "      <td>пример</td>\n",
       "      <td>владимир</td>\n",
       "      <td>применение</td>\n",
       "      <td>работать</td>\n",
       "      <td>сеть</td>\n",
       "      <td>собираться</td>\n",
       "      <td>сеть</td>\n",
       "      <td>момент</td>\n",
       "      <td>подробно</td>\n",
       "      <td>хабра</td>\n",
       "      <td>дания</td>\n",
       "      <td>задумываться</td>\n",
       "      <td>беларусь</td>\n",
       "      <td>практика</td>\n",
       "      <td>чужбина</td>\n",
       "      <td>переехать</td>\n",
       "      <td>япония</td>\n",
       "      <td>разработчик</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>метод</td>\n",
       "      <td>специалист</td>\n",
       "      <td>использование</td>\n",
       "      <td>студент</td>\n",
       "      <td>искусственный</td>\n",
       "      <td>департамент</td>\n",
       "      <td>обычно</td>\n",
       "      <td>запустить</td>\n",
       "      <td>считаться</td>\n",
       "      <td>выход</td>\n",
       "      <td>рассказать</td>\n",
       "      <td>статья</td>\n",
       "      <td>дать</td>\n",
       "      <td>переезд</td>\n",
       "      <td>подумать</td>\n",
       "      <td>уважаемый</td>\n",
       "      <td>несколько</td>\n",
       "      <td>небольшой</td>\n",
       "      <td>начать</td>\n",
       "      <td>программный</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0           1              2         3              4  \\\n",
       "0      привет    валентин  распознавание    привет       делиться   \n",
       "1       хабра    евтюхина    изображение     хабра        рассказ   \n",
       "2     сегодня       автор   классический     звать           опыт   \n",
       "3  рассказать       канал         пример  владимир     применение   \n",
       "4       метод  специалист  использование   студент  искусственный   \n",
       "\n",
       "             5           6                7          8               9  \\\n",
       "0        звать    слышимый          неважно       пять        сердечно   \n",
       "1         иван  сверточный  зарекомендовать      назад  приветствовать   \n",
       "2        серов   нейронный         компания  нейронный     хабравчанин   \n",
       "3     работать        сеть       собираться       сеть          момент   \n",
       "4  департамент      обычно        запустить  считаться           выход   \n",
       "\n",
       "           10      11           12            13        14           15  \\\n",
       "0      привет    пара  продолжение      довольно  недавний       вообще   \n",
       "1        пост   месяц        серия    специалист    статья   нормальный   \n",
       "2      хотеть   назад      рассказ     индустрия   переезд  общемировой   \n",
       "3    подробно   хабра        дания  задумываться  беларусь     практика   \n",
       "4  рассказать  статья         дать       переезд  подумать    уважаемый   \n",
       "\n",
       "          16         17       18           19  \n",
       "0  очередной     привет  попытка     компания  \n",
       "1    история  одесситка    найти     наиболее  \n",
       "2    переезд      назад   работа    известный  \n",
       "3    чужбина  переехать   япония  разработчик  \n",
       "4  несколько  небольшой   начать  программный  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('value.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "for col in df.columns:\n",
    "    #documents.append([word for word in df[col] if str(word) != 'nan'])\n",
    "    documents.append(df[col].dropna().tolist())\n",
    "#print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(4493 unique tokens: ['кат', 'повредить', 'помогать', 'падение', 'интервал']...)\n"
     ]
    }
   ],
   "source": [
    "dct = corpora.Dictionary(documents)\n",
    "dct.save('deerwester.dict')\n",
    "print(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dct.doc2bow(doc) for doc in documents]\n",
    "corpora.MmCorpus.serialize('deerwester.mm', corpus)  # store to disk, for later use\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus, id2word = dct, num_topics = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "#stop_words = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    #tokens = text.rstrip()\n",
    "    tokens = re.sub(\"[^\\w]\", \" \", text).split()\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tokens = [morph.parse(word)[0].normal_form for word in tokens]\n",
    "    #tokens = [i for i in tokens if (i not in stop_words)]    \n",
    "    return ' '.join(tokens)\n",
    "def make_doc(doc):\n",
    "    prog = re.compile('[А-Яа-я]+')\n",
    "    doc = prog.findall(doc.lower())\n",
    "    doc = [morph.parse(token)[0].normal_form for token in doc]\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('query.json', 'r', encoding = 'utf8') as file:\n",
    "    query = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "То, что делала моя новая команда охватывало значительно больше областей, нежели системное администрирование, которым я занимался ранее. У нас был свой проект, в котором мы отвечали за всё: архитектуру, разработку, поддержку и даже продвижение. Для меня это было невероятно. Происходящее было похоже на бесконечный стартап. Большое количество креатива и челенджей каждый день. Из того, с чем мне нужно было работать я знал практически ничего, и только неплохое знание Linux позволило мне начать вникать. Три месяца у меня длился испытательный срок, который был скорее формальностью. На этот срок мне был назначен ментор, который ставил мне задачи и помогал со сложными вопросами. Так же в течении этого срока мне нужно было сдать ещё один тест по английскому (TOEIC IP, достаточно простой). В нашей команде есть менеджер (решающий в основном организационные вопросы и оказывающий всю возможную поддержку команде) и тимлид (или техлид). Никаких разграничений касательно того кто и что делает у нас нет. Работает это примерно так: где-то раз в полгода мы собираем идеи, расставляем приоритеты, оформляем их в новые проекты и закрепляем (обычно волонтерски) каждый из них за одним из членов команды как за ответственным. Работать над проектом, при этом, может кто угодно из команды. Затем проекты разбиваются на задачи, и кто-то берётся (обычно волонтерски) эти задачи делать. Вся эта система нестрогая и время от времени мы меняем те или иные правила, если замечаем, что что-то уже не работает или находим более эффективные способы работы.\n"
     ]
    }
   ],
   "source": [
    "print(query['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = query['text']\n",
    "doc = tokenize_text(doc)\n",
    "doc = make_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = corpora.UciCorpus(\"docword.kek_test.txt\", \"vocab.kek_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary2 = data.create_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3.2473692445949323), (1, 0.026864904498103002)]\n"
     ]
    }
   ],
   "source": [
    "vec_bow = dct.doc2bow(doc)\n",
    "vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from gensim.similarities import MatrixSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = similarities.MatrixSimilarity(lsi[corpus]) # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.80472738), (1, 0.954238), (2, 0.82118046), (3, 0.89350337), (4, 0.89549196), (5, 0.95630997), (6, 0.80542248), (7, 0.69061613), (8, 0.86199248), (9, 0.86598402), (10, 0.83523512), (11, 0.92778146), (12, 0.94807297), (13, 0.94758844), (14, 0.97121191), (15, 0.96160966), (16, 0.92102939), (17, 0.96301025), (18, 0.94357127), (19, 0.99969012)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(19, 0.99969012), (14, 0.97121191), (17, 0.96301025), (15, 0.96160966), (5, 0.95630997), (1, 0.954238), (12, 0.94807297), (13, 0.94758844), (18, 0.94357127), (11, 0.92778146), (16, 0.92102939), (4, 0.89549196), (3, 0.89350337), (9, 0.86598402), (8, 0.86199248), (10, 0.83523512), (2, 0.82118046), (6, 0.80542248), (0, 0.80472738), (7, 0.69061613)]\n"
     ]
    }
   ],
   "source": [
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims) # print sorted (document number, similarity score) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from gensim.similarities import MatrixSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Добавление в модель новых документов, содержащихся в новом корупсе data2\n",
    "ldamodel.update(data2, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = [(1, 5555)]\n",
    "index = MatrixSimilarity(data, num_features=len(dictionary))\n",
    "sims = index[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.01229519  0.01107698  0.          0.          0.          0.          0.\n",
      "  0.00868271  0.0246782   0.          0.          0.01163184  0.01477797\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.017702214), (3, 0.97220784)]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Получение распределения тем для конкретного документа\n",
    "doc = list(data)[0]\n",
    "ldamodel.get_document_topics(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from scipy import spatial\n",
    "#from PhyVariables import PhyVariables\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PhyVariables():\n",
    "    def __init__(self):\n",
    "        self.save_folder_key = 'test_2_urls'\n",
    "        self.articles_json_key = 'articles.json'\n",
    "        self.query_json_key = 'query.json'\n",
    "\n",
    "        self.docword_articles_key = '/docword.articles.txt'\n",
    "        self.vocab_articles_key = '/vocab.articles.txt'\n",
    "        self.docword_query_key = '/docword.query.txt'\n",
    "        self.vocab_query_key = '/vocab.query.txt'\n",
    "        self.model_name_key = '/ldamodel_xkcd_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Косинусное расстояние - чем больше, тем лучше\n",
    "def cos(x, y):\n",
    "    return x.dot(y) / (np.sqrt(x.dot(x)) * np.sqrt(y.dot(y)))\n",
    "\n",
    "\n",
    "def to_vec(x):\n",
    "    kl = np.zeros(number_of_topics)\n",
    "    # kl = [.0,.0,.0,.0,.0,.0,.0,.0,.0,.0]\n",
    "    for i in range(len(x)):\n",
    "        j = x[i][0]\n",
    "        kl[j] = x[i][1]\n",
    "    return np.array(kl)\n",
    "\n",
    "\n",
    "def print_top_words():\n",
    "    all_words = []\n",
    "    for i in range(number_of_topics):\n",
    "        rus_words = []\n",
    "        words = ldamodel.show_topic(i, 10)\n",
    "        words = np.take(words, [0], axis=1)\n",
    "        for w in words:\n",
    "            rus_words.append(w[0].decode('utf8'))\n",
    "        all_words.append(rus_words)\n",
    "\n",
    "    for i, w in enumerate(all_words):\n",
    "        print(\"тема\", i, \":\", w)\n",
    "\n",
    "\n",
    "def calculate_perplexity(model):\n",
    "    model.log_perplexity(list(data))\n",
    "    perp = ldamodel.bound(data)\n",
    "    return 2 ** (-perp / float(87409))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_2_urls/docword.articles.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8bbd37355979>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mphy_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhyVariables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphy_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_folder_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUciCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mphy_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocword_articles_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mphy_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_articles_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\pokus\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\ucicorpus.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fname, fname_vocab)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \"\"\"\n\u001b[0;32m    182\u001b[0m         \u001b[0mIndexedCorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mUciReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfname_vocab\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\pokus\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\ucicorpus.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_nnz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\pokus\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEFAULT_ERRORS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"s3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"s3n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m's3u'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0ms3_open_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\pokus\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mfile_smart_open\u001b[1;34m(fname, mode, encoding, errors)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mraw_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m     \u001b[0mraw_fobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m     \u001b[0mdecompressed_fobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompression_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_fobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m     \u001b[0mdecoded_fobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoding_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecompressed_fobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_2_urls/docword.articles.txt'"
     ]
    }
   ],
   "source": [
    "number_of_topics = 2\n",
    "ldamodel = models.ldamodel.LdaModel(data, id2word=dictionary, num_topics=number_of_topics, passes=20, alpha=1.25, eta=1.25)\n",
    "ldamodel.save(path + phy_var.model_name_key)\n",
    "# Load model:\n",
    "# ldamodel = models.ldamodel.LdaModel.load(path+phy_var.model_name_key)\n",
    "\n",
    "print_top_words()\n",
    "\n",
    "perplexity = calculate_perplexity(ldamodel)\n",
    "print('\\nperplexity = ', perplexity)\n",
    "\n",
    "data2 = corpora.UciCorpus(path + phy_var.docword_query_key, path + phy_var.vocab_query_key)\n",
    "\n",
    "P_tema_query = ldamodel.get_document_topics(list(data2)[0])\n",
    "print('\\nраспределение тем для запроса: \\n', P_tema_query)\n",
    "\n",
    "P_tema_query_vec = []\n",
    "for i in range(0, len(P_tema_query)):\n",
    "    P_tema_query_vec.append(P_tema_query[i][1])\n",
    "print('\\nраспределение тем для запроса в виде вектора:\\n', P_tema_query_vec)\n",
    "\n",
    "cosines = []\n",
    "for i in range(2):\n",
    "    P_tema_articles = ldamodel.get_document_topics(list(data)[i])\n",
    "    P_tema_articles_vec = to_vec(P_tema_articles)\n",
    "    sim = 1 - spatial.distance.cosine(P_tema_articles_vec, P_tema_query_vec)\n",
    "    cosines.append(sim)\n",
    "\n",
    "print('\\nкосинусы:', cosines)\n",
    "print('\\nискомая статья - ', cosines.index(max(cosines)) + 1, 'я')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
